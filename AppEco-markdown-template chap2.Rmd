---
title: "Intro to Time-Series"
subtitle: "Applied Econometrics for Finance"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      fig.align='center')
```
# Random process vs Time-Series

```{r,echo=TRUE, fig.height=6, fig.width=8}
nb.obs <- 252
nb.var <- 4
e <- replicate(n = nb.var,
expr = sample(c(-1, 1),
size = nb.obs, replace = TRUE,
prob = c(0.5, 0.5)))
y <- matrix(data = 0, nb.obs, nb.var)
for(i in 1:nb.var){
for(t in 2:nb.obs){
y[t, i] = y[t - 1, i] + e[t, i]
}
}
par(mfrow = c(2, 2))
for(i in 1:4){
plot(y[,i], type = "l", ylab = "y[t]", xlab = "time")
abline(h = 0, col = "blue")
}

```
# Autocorrelation functionsof time-Series

Proposition: The autocorrelation function of a time-series gives key insights about the properties of the underlying *data generating process*(i.e. random process)

Below, we generate time-series from three random processes and analyze their ACF's :

```{r, fig.height=9, fig.width=8}
# sample size
nb.obs <- 1000
# Generating time-series
y1 <- arima.sim(n = nb.obs,
list(order = c(1,0,0), ar = c(0.99)))
y2 <- arima.sim(n = nb.obs,
list(order = c(0,0,0)))
y3 <- arima.sim(n = nb.obs,
list(order = c(2,0,0), ar = c(0.25, 0.1)))
# visualize output
par(mfrow = c(3,2)) # 3 by 2 grid for ploy layout
plot.ts(y1, ylab = expression(y[t]), main = "Time-series")
acf(y1, lag.max =  12, lwd = 2, main = "ACF")
plot.ts(y2, ylab = expression(y[t]), main = "Time-series")
acf(y2, lag.max = 12, lwd = 2, main = "ACF")
plot.ts(y3, ylab = expression(y[t]), main = "Time-series")
acf(y3, lag.max = 12, lwd = 2, main = "ACF")

```


For example, the first 12 autocorrelations of the series $y2$ are : 
```{r}
acf(y2, lag.max = 12, lwd = 2, main = "ACF", plot=FALSE)$acf
```
Observations

* The ACF of the series $y1$ is typical of that of a *non-stationary process* : The sample autocorrelations remain significant and persistent

* The ACF of the second and third series appear to belong to a *stationary process* with same significant serial correlation for $y3$.

* with $y2$, no significant serial correlation at lags 1, 2,...: current values *uncorrelated* with past values

* with *y3* , data appear to be stationary (horizental) but correlated with the past up to 
2 lags. Q : How to exploit this dependence in a statistical model for $y3$?

# Testing autocorrelation

## Testing individual autocorrelation coefficients 
```{r}
data <- read.csv(
          file.path(
            "https://raw.githubusercontent.com/erkind/",
            "AppEco/main/m-FF-funds-0518.csv"),
          header = TRUE, sep = ",")

```


```{r}
data_subset <- data[,c(2:4,18,19,20)]
dim(data_subset)
colnames(data_subset)
```

```{r,  fig.height=9, fig.width=8}
par(mfrow = c(3,2))
for(i in 1:6){
  plot.ts(data_subset[,i], main = colnames(data_subset)[i],
          ylab = "returns", xlab = "time")  
}
```

Observation: All the six time-series appear to be stationary; let's check their sample ACF.

```{r, fig.height=9, fig.width=8}
par(mfrow = c(3,2))
for(i in 1:6){
  acf(data_subset[,i], main = colnames(data_subset)[i],
      lag.max = 12, type = c('correlation'), ylab = "sample ACF",
      plot = TRUE, lwd = 2)
}
```


*Exercice 1*: Extract the first 6 autocorrelations of the market risk premium. What is
the value of lag-1 sample ACF? Using the large sample distribution of sample ACF, test if
the lag-1 ACF is significantly different from 0. Here, it's 1% significance.
```{r}
acf_test <- acf(data_subset[,1], lag.max = 6,
                type = c('correlation'), plot=FALSE)
```

Lag-1 sample ACF is 
```{r}
acf_test$acf[2]
```
The $t$-statistic is equal to 'sample lag-1 ACF' $\times$ 'sqrt(sample size)'

```{r}
t_stat_rho1 <- acf_test$acf[2] * (length(data_subset$MKT_RF)^0.5)
t_stat_rho1
```
Reading the Student's t-table
```{r}
T <- length(data_subset$MKT_RF)
qt(0.995, df = T - 1)
qt(0.005, df = T - 1)
```


Decision : The critical values are the 0.5th and 99.5th percentiles of t-distribution at
$T -1$ degrees-of-freedom. Using R (do it yourself using R  function 'qt() '), these
values can be found as $-2.61$ and $2.61$. The test statistic lies outside the critical region, so $H_0$ cannot be rejected.

*Exercice 2*: Perform the same steps for testing lag-1 ACF of HML factor. Use 5\% 
significance level to drive the critical values of the test. State your decision.

```{r}
acf_test_HML <- acf(data_subset[,3], lag.max = 6,
                type = c('correlation'), plot=FALSE)
```


```{r}
t_stat_rho2_HML <- acf_test_HML$acf[2] * (length(data_subset$HML)^0.5)
t_stat_rho2_HML
```


Reading the Student's t-table with 

```{r}
T <- length(data_subset$HML)
qt(0.975, df = T - 1)
qt(0.025, df = T - 1)
```
Decision : The critical values are the 2.5th and 97.5th percentiles of t-distribution at
$T -1$ degrees-of-freedom. Using R (do it yourself using R  function 'qt() '), these
values can be greater than $1.97$. The test statistic lies within the critical region, so $H_0$ is rejected.



*Exercise 3* : Using the 'Box.test()' function, test the joint significance of the first 12 ACF's of the returns on VSGIX and VSIIX. using the p-value of the output, state your decision at 5\% significiance level. 


```{r}
Box.test(data_subset$VSGIX, lag = 12, type = c('Ljung-Box'))
Box.test(data_subset$VSIIX, lag = 12, type = c('Ljung-Box'))
```



